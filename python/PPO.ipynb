{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"boatsimulator-05\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 5000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"boatsimulator\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 5000 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 512 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'learning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'BoatAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: BoatAcademy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BoatBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 9\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 2\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000. Mean Reward: 2.7726151048239984. Std of Reward: 0.005823020012774545.\n",
      "Step: 10000. Mean Reward: 2.7621021997070008. Std of Reward: 0.0062749916298556535.\n",
      "Step: 15000. Mean Reward: 2.7616216547969996. Std of Reward: 0.006210080470399283.\n",
      "Step: 20000. Mean Reward: 2.7640667624520003. Std of Reward: 0.009865770582925927.\n",
      "Step: 25000. Mean Reward: 2.749715102074002. Std of Reward: 0.01227585029907722.\n",
      "Step: 30000. Mean Reward: 2.7454754368900036. Std of Reward: 0.01752199591482702.\n",
      "Step: 35000. Mean Reward: 2.7392683735779992. Std of Reward: 0.019022209819430116.\n",
      "Step: 40000. Mean Reward: 2.745370649786001. Std of Reward: 0.013136083040461185.\n",
      "Step: 45000. Mean Reward: 2.7366688620599957. Std of Reward: 0.013292285969554457.\n",
      "Step: 50000. Mean Reward: 2.7275013075050003. Std of Reward: 0.009561457849639663.\n",
      "Saved Model\n",
      "Step: 55000. Mean Reward: 2.7345607862310004. Std of Reward: 0.01189582948907938.\n",
      "Step: 60000. Mean Reward: 2.7192218287979997. Std of Reward: 0.019821405817996282.\n",
      "Step: 65000. Mean Reward: 2.734851830678001. Std of Reward: 0.010696770528169584.\n",
      "Step: 70000. Mean Reward: 2.742096325385. Std of Reward: 0.014236233973365557.\n",
      "Step: 75000. Mean Reward: 2.7401134194349996. Std of Reward: 0.01125994003499806.\n",
      "Step: 80000. Mean Reward: 2.7429344433500003. Std of Reward: 0.019271330937016995.\n",
      "Step: 85000. Mean Reward: 2.7435655580699994. Std of Reward: 0.013512516881415788.\n",
      "Step: 90000. Mean Reward: 2.7504607830889998. Std of Reward: 0.012883584395329765.\n",
      "Step: 95000. Mean Reward: 2.764256302586001. Std of Reward: 0.016105687170741553.\n",
      "Step: 100000. Mean Reward: 2.768478399847. Std of Reward: 0.013964646010725258.\n",
      "Saved Model\n",
      "Step: 105000. Mean Reward: 2.7575295841569996. Std of Reward: 0.018702043218307547.\n",
      "Step: 110000. Mean Reward: 2.7710770187840015. Std of Reward: 0.009685599872758927.\n",
      "Step: 115000. Mean Reward: 2.772821303134001. Std of Reward: 0.0075752546244205665.\n",
      "Step: 120000. Mean Reward: 2.775139686157. Std of Reward: 0.005754960046343079.\n",
      "Step: 125000. Mean Reward: 2.7774111907480012. Std of Reward: 0.006714388301239735.\n",
      "Step: 130000. Mean Reward: 2.7773900382630012. Std of Reward: 0.006680821898673344.\n",
      "Step: 135000. Mean Reward: 2.7835176717699994. Std of Reward: 0.005815320685467614.\n",
      "Step: 140000. Mean Reward: 2.7859916727380005. Std of Reward: 0.008492367264567612.\n",
      "Step: 145000. Mean Reward: 2.7875246424979947. Std of Reward: 0.009257375883477513.\n",
      "Step: 150000. Mean Reward: 2.782443629781999. Std of Reward: 0.0067501366651547805.\n",
      "Saved Model\n",
      "Step: 155000. Mean Reward: 2.7741152519280012. Std of Reward: 0.013861865772415329.\n",
      "Step: 160000. Mean Reward: 2.7986077799770013. Std of Reward: 0.013318050743085572.\n",
      "Step: 165000. Mean Reward: 2.795915981428. Std of Reward: 0.007682234366484179.\n",
      "Step: 170000. Mean Reward: 2.788566541708. Std of Reward: 0.014319705047081197.\n",
      "Step: 175000. Mean Reward: 2.783085305503. Std of Reward: 0.014733067744035292.\n",
      "Step: 180000. Mean Reward: 2.7808075214940007. Std of Reward: 0.017742523829560687.\n",
      "Step: 185000. Mean Reward: 2.7817817467420007. Std of Reward: 0.011387301984990609.\n",
      "Step: 190000. Mean Reward: 2.792015767499999. Std of Reward: 0.01107529795209139.\n",
      "Step: 195000. Mean Reward: 2.781650707563999. Std of Reward: 0.013230646653739223.\n",
      "Step: 200000. Mean Reward: 2.790715119344. Std of Reward: 0.011220949627517263.\n",
      "Saved Model\n",
      "Step: 205000. Mean Reward: 2.790608060256999. Std of Reward: 0.013978534926076137.\n",
      "Step: 210000. Mean Reward: 2.7895080055860006. Std of Reward: 0.006906981208051044.\n",
      "Step: 215000. Mean Reward: 2.7903356618299977. Std of Reward: 0.009868805742748923.\n",
      "Step: 220000. Mean Reward: 2.7881367341590018. Std of Reward: 0.010567191833937404.\n",
      "Step: 225000. Mean Reward: 2.7926232874040005. Std of Reward: 0.009653382395607719.\n",
      "Step: 230000. Mean Reward: 2.7954356035780012. Std of Reward: 0.013362225529216298.\n",
      "Step: 235000. Mean Reward: 2.7887508074989995. Std of Reward: 0.011371668222960048.\n",
      "Step: 240000. Mean Reward: 2.798866956091. Std of Reward: 0.006522698727596768.\n",
      "Step: 245000. Mean Reward: 2.793755361374. Std of Reward: 0.009796033542111365.\n",
      "Step: 250000. Mean Reward: 2.8048138174409103. Std of Reward: 0.005297620573981652.\n",
      "Saved Model\n",
      "Step: 255000. Mean Reward: 2.8043944785839994. Std of Reward: 0.008686551401168564.\n",
      "Step: 260000. Mean Reward: 2.8082775041490002. Std of Reward: 0.010115349780483946.\n",
      "Step: 265000. Mean Reward: 2.809438839562. Std of Reward: 0.006534053509621002.\n",
      "Step: 270000. Mean Reward: 2.820800095681999. Std of Reward: 0.009834040075555243.\n",
      "Step: 275000. Mean Reward: 2.8289958927449996. Std of Reward: 0.009616091542937247.\n",
      "Step: 280000. Mean Reward: 2.823995827316. Std of Reward: 0.013196679042438809.\n",
      "Step: 285000. Mean Reward: 2.8252880284829995. Std of Reward: 0.01009551615392417.\n",
      "Step: 290000. Mean Reward: 2.8234118584480004. Std of Reward: 0.01264622789901381.\n",
      "Step: 295000. Mean Reward: 2.8364759172920007. Std of Reward: 0.026045587854222133.\n",
      "Step: 300000. Mean Reward: 2.882192740106001. Std of Reward: 0.02864777784228645.\n",
      "Saved Model\n",
      "Step: 305000. Mean Reward: 2.8639339501360004. Std of Reward: 0.018451628965268283.\n",
      "Step: 310000. Mean Reward: 2.872028659943001. Std of Reward: 0.026667110576075338.\n",
      "Step: 315000. Mean Reward: 2.8573809706299995. Std of Reward: 0.02385151951137431.\n",
      "Step: 320000. Mean Reward: 2.8694573616809995. Std of Reward: 0.020753656410625213.\n",
      "Step: 325000. Mean Reward: 2.872820927068. Std of Reward: 0.03225765928211268.\n",
      "Step: 330000. Mean Reward: 2.8888513448199995. Std of Reward: 0.031346611335452586.\n",
      "Step: 335000. Mean Reward: 2.893780466903. Std of Reward: 0.036680228659918275.\n",
      "Step: 340000. Mean Reward: 2.8765514606749996. Std of Reward: 0.0152639870044714.\n",
      "Step: 345000. Mean Reward: 2.8528909904570003. Std of Reward: 0.03247753589731003.\n",
      "Step: 350000. Mean Reward: 2.889243453051999. Std of Reward: 0.031543818809704124.\n",
      "Saved Model\n",
      "Step: 355000. Mean Reward: 2.906102873651. Std of Reward: 0.023686894893070125.\n",
      "Step: 360000. Mean Reward: 2.9498181168159996. Std of Reward: 0.054588277136062735.\n",
      "Step: 365000. Mean Reward: 2.9137765124520003. Std of Reward: 0.03987068662832089.\n",
      "Step: 370000. Mean Reward: 2.9195087838180003. Std of Reward: 0.055373510795078455.\n",
      "Step: 375000. Mean Reward: 2.8986043189900004. Std of Reward: 0.025054866856820235.\n",
      "Step: 380000. Mean Reward: 2.919425605503. Std of Reward: 0.03907602610784829.\n",
      "Step: 385000. Mean Reward: 2.89469410212. Std of Reward: 0.03145832440087861.\n",
      "Step: 390000. Mean Reward: 2.886753844338. Std of Reward: 0.02220815266552936.\n",
      "Step: 395000. Mean Reward: 2.888170794443. Std of Reward: 0.032374236449243116.\n",
      "Step: 400000. Mean Reward: 2.889831514537001. Std of Reward: 0.01575446946252664.\n",
      "Saved Model\n",
      "Step: 405000. Mean Reward: 2.8596211979090005. Std of Reward: 0.024993031306500397.\n",
      "Step: 410000. Mean Reward: 2.860321758772001. Std of Reward: 0.02714677595536242.\n",
      "Step: 415000. Mean Reward: 2.8823999229439985. Std of Reward: 0.029631623376831492.\n",
      "Step: 420000. Mean Reward: 2.8663788086119992. Std of Reward: 0.023338697771174956.\n",
      "Step: 425000. Mean Reward: 2.8482500444320005. Std of Reward: 0.009720440002700602.\n",
      "Step: 430000. Mean Reward: 2.8498108267809994. Std of Reward: 0.019173607664689226.\n",
      "Step: 435000. Mean Reward: 2.8556563006649993. Std of Reward: 0.013257370590462843.\n",
      "Step: 440000. Mean Reward: 2.8500238235090003. Std of Reward: 0.021198511516750593.\n",
      "Step: 445000. Mean Reward: 2.8496330511430004. Std of Reward: 0.029990023179322742.\n",
      "Step: 450000. Mean Reward: 2.8552627001959996. Std of Reward: 0.013955487105280835.\n",
      "Saved Model\n",
      "Step: 455000. Mean Reward: 2.8364569327109996. Std of Reward: 0.026155293040758063.\n",
      "Step: 460000. Mean Reward: 2.8627968785610007. Std of Reward: 0.028636297041741626.\n",
      "Step: 465000. Mean Reward: 2.8676960255359982. Std of Reward: 0.028009225259830208.\n",
      "Step: 470000. Mean Reward: 2.8539981428349988. Std of Reward: 0.02093268333242926.\n",
      "Step: 475000. Mean Reward: 2.8564206493969997. Std of Reward: 0.018228456988233374.\n",
      "Step: 480000. Mean Reward: 2.854488126045999. Std of Reward: 0.013163723094182938.\n",
      "Step: 485000. Mean Reward: 2.8585937545249993. Std of Reward: 0.01940942252148256.\n",
      "Step: 490000. Mean Reward: 2.8524760917440006. Std of Reward: 0.01687780573687588.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 495000. Mean Reward: 2.850352287974. Std of Reward: 0.010396431489456417.\n",
      "Step: 500000. Mean Reward: 2.8554529121445453. Std of Reward: 0.01699769982579453.\n",
      "Saved Model\n",
      "Saved Model\n",
      "INFO:tensorflow:Restoring parameters from ./models/boatsimulator-05/model-500001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/boatsimulator-05/model-500001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/boatsimulator-05/model-500001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/boatsimulator-05/model-500001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
